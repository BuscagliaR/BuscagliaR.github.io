Joint Distribution
========================================
date: 3/22/2020
font-family: 'Tahoma'
transition-speed: fast
width: 1440
height: 1250

Section 4.3

Joint Distribution
========================================================

Observe the value of 2 (or more) random variables and determine their joint probability structure.

**Example 1.** Roll a fair 4-sided die twice, record $X$ as the outcome of the first roll and $Y$ the outcome of the second.

The set of all possibilities is the **joint support** for $(X,Y)$.

$$
\begin{aligned}
(x, y) \in \{&(1,1), (1,2), (1,3), (1,4), \\
&... \\
&(4,1), (4,2), (4, 3), (4,4)\}
\end{aligned}
$$

Bivariate Random Variable
========================================================
type: sub-section

*Definition.* 

The ordered pair of random variables $(X, Y)$ is a **bivariate random variable** (or random vector) if each $X$ and $Y$ associates a real number with every element of a sample space $\Omega$.

We can extend this definition to *multivariate* rather than just bivariate.

Joint pmf
========================================================
type: sub-section

*Definition.* 

For $X$ and $Y$ both discrete random variables, the **joint probability mass function** (joint pmf, bivariate pmf) for $(X,Y)$ is

$$
p(x,y) = P(X=x, Y=x)
$$

Bivariate Example
========================================================

From Example 1, since the distribution is uniform, we can write the joint pmf as

$$
p(x,y) = 
\begin{cases} 
    1/16 & x,y \in \{1,2,3,4\} \\
    0 & else
\end{cases}
$$

It can also be important to graph the support of the probability distribution.  Here, we graph the joint support.

<div align="center">
<img src="Ex1_Joint_Support.jpg" width=500 height=250>
</div>

Joint pmf Example
========================================================
incremental: true

**Example 2.**

Roll a 6-sided fair die once, record as $X$.  What is the distribution and support of $X$?

$X \sim Unif(S_X)$ where $S_X = \{1, 2, ..., 6\}$.

Deal $X$ cards from a standard deck and record $Y$ the number of aces.  What is the support of $Y$?

$$
S_Y = \begin{cases} 
    0, 1, ..., x & x < 4 \\
    0, 1, ..., 4 & x \geq 4
\end{cases}
$$


Example 2 Joint Support
========================================================
incremental: false

This is certainly a different type of joint support.  We can draw it out.

<div align="center">
<img src="Ex2_Joint_Support.jpg" width=500 height=300>
</div>

Observe that the support of the joint distribution is no longer the Cartesian product of the two random variables $X$ and $Y$.  This is a necessary condition for Independence (see Section 4.4).

Example 2 Joint pmf
========================================================
incremental: true

How can we study the joint pmf for this problem?

We should first observe that we can calculate these probabilities by using condition.  Observe,

$$\small p(1,0) = P(X=1, Y=0) = P(Y=0|X=1)P(X=1)$$

Joint pmfs give the probabilities of observing the intersection of two random variable outcomes, thus we can apply our conditioning similar to Chapter 2.

We have defined $X$ as a uniform distribution and so $\small P(X=x) = 1/6$ for $\small x \in S_X$.

Example 2 Joint pmf Continued
========================================================
incremental: true

How do we find $\small P(Y=0|X=1)$?

Counting!

$$
\small
P(Y=0|X=1) = \frac{{4 \choose 0}{48 \choose 1}}{{52 \choose 1}}
$$

We can then find $\small p(1,0)$.

$$
\small
\begin{aligned}
p(1,0) &= P(X=1, Y=0) \\
&= P(Y=0|X=1)P(X=1) \\
&= \frac{{4 \choose 0}{48 \choose 1}}{{52 \choose 1}}(1/6) = 2/13
\end{aligned}
$$

Example 2 Joint pmf Continued
========================================================
Similarly $\small p(1,1)$.

$$
\small
\begin{aligned}
p(1,1) &= P(X=1, Y=1) \\
&= P(Y=1|X=1)P(X=1) \\
&= \frac{{4 \choose 1}{48 \choose 0}}{{52 \choose 1}}(1/6) = 1/78
\end{aligned}
$$

We could continue this for all possible $(x,y)$ pairs.

Example 2 Joint pmf
========================================================
We may also at this stage be able recognize a generalization of the formula. We need to be clear about our support of $Y$ though, since it depends on $X$.

$$
\small
\begin{aligned}
P(X=x, Y=y) = \begin{cases} 
    \frac{{4 \choose y}{48 \choose x-y}}{{52 \choose x}}(1/6) & \text{for } x = 1, 2, 3 \text{ and } y = 0, 1, ..., x \\
    \frac{{4 \choose y}{48 \choose x-y}}{{52 \choose x}}(1/6) & \text{for } x = 4, 5, 6 \text{ and } y = 0, 1, ..., 4 \\
    0 & else
\end{cases}
\end{aligned}
$$

The need to write it this way is because $Y$ only depends on $X$ for values of $X$ less than 4.  For all outcomes of $X$ greater than 4, it is possible to see all outcomes of $Y$, since there are only 4 aces in the deck.

Marginal pmf
========================================================
type: sub-section

*Definition.* 

For a bivariate random variable $(X,Y)$ with joint pmf

$$
\small
p_{X,Y}(x,y) = P(X=x, Y=y)
$$

the **marginal probability mass function** for $X$ is given by

$$
\small
p_{X}(x) = P(X=x) = \sum_{y\in S_Y}p_{X,Y}(x,y)
$$

Similarly, we can define the marginal pmf for $Y$ by

$$
\small
p_{Y}(y) = P(Y=y) = \sum_{x\in S_X}p_{X,Y}(x,y)
$$

Example 3 Marginal pmf
========================================================
incremental: false

Let $\small x \in \{1,2,3\}$ and $\small x \in \{1,2,3\}$ be outcomes of the bivariate random variable $\small (X,Y)$.  The table below gives the joint pmf for this bivariate random variable.

<div align="center">
<img src="Ex3_p1.jpg" width=500 height=250>
</div>

The table gives the probability of seeing each bivariate $\small (x,y)$ pair.  Thus, for example, observe $\small p(1,5) = 1/6$.

Example 3 Marginal pmf
========================================================
incremental: true

How do we get the marginal pmfs for $X$ and $Y$?

Recall $\small p_{X}(x) = P(X=x) = \sum_{y\in S_Y}p_{X,Y}(x,y)$.  Thus, if we find all of the column sums, we obtain the marginal pmf for $X$.  If we find the row sums, we find the marginal pmf for $Y$.

![The marginal pmfs for Example 3.](Ex3_p2.jpg)

Observe that $\small p_X(x) = 1/3$ for each value of $x$, thus we can say the marginal pmf of $X$ is uniform.  This is displayed as the 4th row.  The 4th column gives the marginal pmf of $Y$.

Expectation of a function of two random variables
========================================================
type: sub-section

*Result.* 

For $\small f(x,y)$ a function of the pair of random variable $\small (X,Y)$, the expected value of $\small f(X,Y)$ is

$$
E[f(X,Y)] = \sum\limits_{x\in S_X} \sum\limits_{y\in S_y} f(x,y)P(X=x,Y=y)
$$

Example 3 Continued
========================================================
incremental: false

Returning to the table given for Example 3, find the $E[X-Y]$.

<div align="center">
<img src="Ex3_p1.jpg" width=500 height=250>
</div>

This is only tedious because we must calculate this over all $(x,y)$ pairs.

$$
\small
\begin{aligned}
E[X-Y] &= \sum\limits_{x=1}^{3} \sum\limits_{y=3}^5 (x-y)P(X=x,Y=y) \\
&= (1-3)p(1,3) + (1-4)p(1,4) + (1-5)p(1,5) + ... + (3-5)p(3,5) \\
&= -2(1/18) - 3(1/9) - 4(1/6) - (1/18) - 2(1/9) \\
&\quad  - 3(1/6) + 0 - (1/9) - 2(1/6) \\
&= -7/3 \approx -2.33
\end{aligned}
$$