Independent Random Variables
========================================
date: `r format(Sys.time(),  '%B %d, %Y')`
font-family: 'Tahoma'
transition-speed: fast
width: 1440
height: 1250

Section 4.4

Independent Events
========================================================
type: sub-section

Recall that two events, A and B, are independent if $\small P(A\cap B) = P(A)P(B)$.  We extend this concept for Random Variables.

*Definition.* 

$\small X$ and $\small Y$ are **independent random variables** if the joint pmf of $\small X$ and $\small Y$ factors into the product of the marginal pmf of $\small X$ times the marginal pmf of $\small Y$.

This can be written

$$
P(X = x, Y=y) = P(X=x)P(Y=y) \quad \text{for all pairs of } (x,y)
$$

or

$$
p_{X,Y}(x,y) = p_X(x)p_Y(y) \quad \text{for all pairs of } (x,y)
$$

Example 1
========================================================

Let $X$ and $Y$ be random variables such that their joint pmf takes on the following table.  Recall from Section 4.3.

<div align="center">
<img src="Ex3_p2.jpg" width=500 height=250>
</div>

Observe that for each $(x,y)$ pair, the product of the marginal pmfs is equivalent to the probability of the joint pmf.  Thus, we can state that in this example $X$ and $Y$ are *independent*.

Independent Random Variables
========================================================
type: sub-section

*Result.* 

A necessary condition for two random variables to be independent is that the joint support for $(X,Y)$ is the Cartesian product of the marginal support of $X$, $S_X$, and the marginal support of $Y$, $S_Y$.  We can write,

$$
S_{X,Y} = S_X \times S_Y
$$

Note that this condition is not sufficient, but gives a quick check for when random variables are dependent.

Independent Random Variables Example
========================================================
incremental: true

**Example 2.**

Suppose $(X,Y)$ has the joint pmf $p(x,y) = cxy$, where $c$ is a real value.  Let $x \in S_X = \{1,2,3\}$ and $y \in S_Y = \{1,2\}$

**I.**  For what value of c is this a well defined pmf?

We use that for any probability distribution, the sum over all observations must equal zero.

$$
\sum\limits_{all x,y}p(x,y) = 1
$$

We are given the possible outcomes for both $X$ and $Y$.  Working out the double some, observe

$$
\begin{aligned}
1 &= \sum\limits_{x=1}^{3}\sum\limits_{y=1}^2cxy = c\sum\limits_{x=1}^{3}\left(x\sum\limits_{y=1}^2y\right) \\
&= c\sum\limits_{x=1}^{3}3x = 3c(6) = 18c \\
\end{aligned}
$$

Thus, we normalize this pmf such that it sums to 1.  This implies that $c = 1/18$.

We can write the joint pmf for $(X,Y)$ as

$$
p_{X,Y}(x,y) = \frac{xy}{18} 1_{S_X}(x) 1_{S_Y}(y)
$$

Example continued
========================================================
incremental: true

**II.** Are $X$ and $Y$ independent?

We can start by checking the necessary condition.  For this problem observe that 

$$
S_{X,Y} = S_X\times S_Y
$$

The joint support is the Cartesian product of the marginal supports, so we have the necessary condition for independence.

Method 1. Verify that $p(x,y) = p_X(x)p_Y(y)$.

We can find the marginal pmfs for $X$ and $Y$ using the definition.

$$
p_X(x) = \sum\limits_{y=1}^{2}\frac{xy}{18} = \frac{x}{6}1_{S_X}(x)
$$

Similarly,

$$
p_Y(y) = \sum\limits_{x=1}^{3}\frac{xy}{18} = \frac{y}{3}1_{S_Y}(y)
$$

Thus, $p(x,y) = p_X(x)p_Y(y)$ for all $(x,y)$ pairs, so this satisfies the definition of independent random variables.

Example continued
========================================================
incremental: true

Method 2.  For this problem, there is a small enough number of pairs, the  pmf can be written as a table.

<div align="center">
<img src="Indep_Table.jpg" width=500 height=250>
</div>

From the table it can be verified that $p(x,y) = p_X(x)p_Y(y)$ for all $(x,y)$ pairs.

How can we change this table to disrupt the independence?

![](Depen_Table.jpg)

This does not satisfy that for each pair $(x,y)$ the joint pmf is not the product of the marginals.

$$
p(3,2) = 7/18 \neq (3/6)(2/3) = p_X(3)p_Y(2)
$$